# Style-Bert-VITS2の音声をファインチューニングについて

Style-Bert-VITS2は日本語に特化した音声生成モデルです。 **GPUによる高速処理が可能** でありながら、Bert(文脈情報)とOpenjTalk(アクセント情報)を使うことで、 **高精度なパフォーマンス** が可能になります。  
しかし、githubで公開されているStyle-Bert-VITS2(音声生成モデル)は未学習モデルです。具体的には、単語のアクセントに誤りが多く、感情的になり過ぎて落ち着きのない音声になっています。  
したがって、自動音声を作成する際には **ファインチューニングが必須** になります。私はファインチューニングしていく過程で、やり方次第で効果や効率が全く異なるということに気づきました。原理的な部分はまだわかっていないことが多いですが、良かったやり方をまとめていきます。

### 1. 声質や口調を模倣するだけなら、非常に効率的に行うことができる
声質や口調を特定の何かに変えたいだけの場合、非常に簡単にファインチューニングすることができます。
具体的には以下の通りです。  
* 訓練データ: 短くて良い。例えば、「おはようございます。今日はいい天気ですね。」で十分。
* パラメータの更新回数: 少なくて良い。2,3epochで十分汎化する。
* 訓練データにノイズが入っていても学習できることが多い。一緒にノイズも学習してしまいますが、声質は学習できます。
* 工夫点: モデルのDecoder(声質を司る層)以外のパラメータを固定すると、アクセント情報や声の安定性が壊れにくい。 ただし、Decoderのパラメータは学習の際に一番最初に動く傾向にあるため、この固定を行わなくても2,3epochで済ませれば問題ない。

### 2. 基本的に汎化性能がある
ファインチューニングで学習できる要素には、声質や口調以外に、アクセントの正確さ、声の安定性などの要素があります。これらは、声質に比べると多くの音声が必要ですが、個人がすぐに用意できる量の音声で十分に汎化します。  
ただし、いくつか大事な条件があるため一緒に書いておきます。

#### 2-1.訓練データはノイズを含んでいてはいけない。できるだけ人間の音声にすること
高性能な音声生成AIがネットにはたくさん転がっています。人間にそっくりです。しかし、音声学的には未だにズレがあります。ニューラルネットワークは規則正しい訓練データの方が学習しやすいという特徴があります。それが原因とは言い切れませんが、人間の音声(もしくは人間だと思ってしまうほどのAI音声)でないと学習は頭打ちになります。

補足:
* ノイズが乗っている場合は、高周波成分をscipyで消すだけでも違いが出ます。
* ノイズを消すAIはたくさんあります。簡単に実装できます。noisereduceなど

#### 2-2. 訓練データは少なくて良いが(最低3分)、あればあるほど良い。

この後の話は、全て**ノイズが乗っていない**ことが前提です。
###### ファインチューニングの大きな効果を感じることができる
* 3分ぐらいのデータで良い。日常的な会話なら、アクセント情報を正しく利用してくれるようにます。また、声に揺らぎがなくなります。
* 200epochほど(400epoch以上まで増やしても無意味)


以下、まだやれていないことです。
###### 大規模な学習(30分ぐらいで十分大規模です。)
* 事前に話者ごとに音声を分離しておきます。これをしないと学習が進みません。speechbrainとfaster_whisperで簡単に実装できます。ただし、話者を分離すれば同時に学習することができ、アクセント情報や文脈情報の活かし方など**日本語の根底にある性質**を同時に学習できるようです。
* 訓練中に損失の値を見て、early-stoppingしましょう。
Style-Bert-VITS2のtrain_ms_jp.pyには損失関数を表示させる仕組みが備わっています。デフォルトでは、コメントアウトされていて、インデントも変になっているので復活させましょう。また、200epochに一回になっているので、config.jsonをいじって、20回に一回ぐらいにしましょう。
* 具体的にはloss_kl(一番左)が下がらなくなったら、学習を中止してください。

### さらに上を目指すなら
* アクセント情報をさらに正確にする。OpenjTalkの情報にはどうしても限界があります。そのため、Mecabのもっと厳密な情報をAttentionして、残差接続する方が汎化性能があるはずです。